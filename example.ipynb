{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f843ef6",
   "metadata": {},
   "source": [
    "### Notebook for presentation at Recognising Refugees: Comparative and Transnational Insights into Asylum under Pressure (Dublin 2025)\n",
    "\n",
    "This notebook provides an example of how to extract information from Canadian refugee law documents at scale using large language models, including both commerical models (OpenAI) and open source models (qwen via ollama)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66ac3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install pandas tqdm datasets openai ollama python-dotenv\n",
    "\n",
    "# general imports\n",
    "from time import sleep\n",
    "\n",
    "# setup pandas\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "# setup access to huggingface data\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84856729",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the data\n",
    "df = load_dataset(\"refugee-law-lab/canadian-legal-data\", data_dir = \"RAD\", split = \"train\").to_pandas()\n",
    "\n",
    "# filter for language == 'en'\n",
    "df = df[df['language'] == 'en']\n",
    "\n",
    "# in df.unofficial_text remove footnotes\n",
    "df['unofficial_text'] = df['unofficial_text'].str.replace(r'\\n(?:\\d+ .+?)(?=\\n)', '', regex=True)\n",
    "\n",
    "# remove header if REASONS FOR DECISION is in the text\n",
    "print(\"Removing header\")\n",
    "def remove_header(text):\n",
    "    before, sep, after = text.partition('REASONS FOR DECISION')\n",
    "    if sep:  # found the string\n",
    "        return sep + after\n",
    "    else:\n",
    "        return text  # leave untouched if not found\n",
    "df['unofficial_text'] = df['unofficial_text'].apply(remove_header)\n",
    "\n",
    "# create sample to get outcomes for all cases\n",
    "df_sample = df.copy()\n",
    "df_sample = df_sample.sample(n=5000, random_state=888).reset_index(drop=True)\n",
    "\n",
    "df.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34395fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keyword search for cases realted to sexual orientation or gendier identity/expression\n",
    "keywords = [\n",
    "    'lgbt',\n",
    "    'queer',\n",
    "    'sogie',\n",
    "    'sexual orienation',\n",
    "    'gay',\n",
    "    'homosexual',\n",
    "    'lesbian',\n",
    "    'bisexual',\n",
    "    'bi-sexual',\n",
    "    'pansexual',\n",
    "    'pan-sexual',\n",
    "    'same-sex',\n",
    "    'same sex',\n",
    "    'transgender',\n",
    "    'transexual',\n",
    "    'transman',\n",
    "    'transwoman',\n",
    "    'mtf',\n",
    "    'ftm',\n",
    "    'non-binary',\n",
    "    'gender identity',\n",
    "    'gender expression',\n",
    "    'genderqueer',\n",
    "    'gender queer',\n",
    "    'gender fluid',\n",
    "    'gender-fluid',\n",
    "    'gender non-conforming',\n",
    "    'gender nonconforming',\n",
    "    'gender expansive',\n",
    "    'two spirit'\n",
    "    ]\n",
    "\n",
    "def keyword_search(text):\n",
    "    for kw in keywords:\n",
    "        if kw in text.lower():\n",
    "            return True\n",
    "    return False\n",
    "print(\"Searching for keywords\")\n",
    "df['keyword_search'] = df['unofficial_text'].progress_apply(keyword_search)\n",
    "\n",
    "print (\"Length of en df before keyword search: \", len(df))\n",
    "df = df[df['keyword_search'] == True]\n",
    "df = df.drop(columns=['keyword_search'])\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "print (\"Length of en df after keyword search: \", len(df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4657c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup ollama\n",
    "# NOTE: Need to install ollama on system: https://ollama.com/download\n",
    "# Set up ollama server (ollama serve), and download the models you want to use (ollama pull <model>)\n",
    "# If you prefer not running ollama server, you can use transformers\n",
    "from ollama import Client as OllamaClient\n",
    "\n",
    "# setup openai\n",
    "# NOTE: Need to create .env file with OPENAI_API_KEY=xxxxxxxx\n",
    "# Get your OpenAI API key from https://platform.openai.com\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# fuctions to call llms\n",
    "\n",
    "def get_ollama_completion(prompt,\n",
    "                 model = 'qwen2.5:72b',\n",
    "                 temperature = 0,\n",
    "                 num_predict = 1,\n",
    "                 host = 'http://ts-ollama:11434',  #For local use, change to 'http://localhost:11434'\n",
    "                 attempts = 3\n",
    "                 ):  \n",
    "\n",
    "    for x in range(attempts):   \n",
    "        try:\n",
    "            client = OllamaClient(host=host)\n",
    "            response = client.generate(\n",
    "                model=model,\n",
    "                prompt=prompt,\n",
    "                options={\"temperature\": temperature, \"num_predict\": num_predict}\n",
    "            )\n",
    "            sleep(.1) # slow down requests to avoid problems with ollama server\n",
    "            return response[\"response\"]\n",
    "    \n",
    "        except:\n",
    "            print(\"Error in connection. Trying again after 10 seconds\")\n",
    "            sleep(10)\n",
    "            if x == attempts - 1:\n",
    "                print(\"Too many errors. Returning empty string.\")\n",
    "                return \"\"\n",
    "\n",
    "\n",
    "def get_openai_completion(user_message,\n",
    "        system_message=\"You are a helpful assistant to a Canadian law student\",\n",
    "        model = \"gpt-4o-mini\",\n",
    "        temperature = 0,\n",
    "        num_predict = 1):\n",
    "    client = OpenAI()\n",
    "    completion = client.chat.completions.create(\n",
    "        model=model,\n",
    "        temperature=temperature,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_message},\n",
    "            {\"role\": \"user\", \"content\": user_message}\n",
    "        ],\n",
    "        max_completion_tokens = num_predict,\n",
    "    )\n",
    "    return completion.choices[0].message.content\n",
    "\n",
    "print(\"Ollama:\", get_ollama_completion(\"If you hear me, say 'working' and nothing elese\"))\n",
    "print(\"OpenAI:\", get_openai_completion(\"If you hear me, say 'working' and nothing elese\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e8de59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up prompts\n",
    "\n",
    "def get_prompt(docs, question = \"Summarize the document.\\n\\n\"):\n",
    "    prompt = f\"\"\"CONTEXT: You are a legal assistant. You are provided a document and you are asked\n",
    "a question about that document. You only answer the question with no explanation\n",
    "\n",
    "DOCUMENT:\n",
    "{docs}\n",
    "\n",
    "QUESTION:\n",
    "{question}\n",
    "\n",
    "ANSWER:\n",
    "\"\"\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d4d7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "outcome_question = \"\"\"True or false: In the document provided, the appeal is granted. Only answer with True or False.\"\"\"\n",
    "\n",
    "# apply to df to get ollama response (use smaller model for speed):\n",
    "df_sample['ollama_outcome'] = df_sample.progress_apply(\n",
    "    lambda x: get_ollama_completion(get_prompt(x['unofficial_text'], outcome_question), model='qwen2.5:32b'),\n",
    "    axis=1)\n",
    "\n",
    "# export to json\n",
    "df_sample.to_json(\"RAD_sample_outcome.json\", orient=\"records\", indent = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33220769",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test for accuracy against openai gpt-4o-mini\n",
    "\n",
    "df_sample = pd.read_json(\"RAD_sample_outcome.json\", orient=\"records\")\n",
    "\n",
    "df_sample = df_sample.sample(100)\n",
    "\n",
    "outcome_question = \"\"\"True or false: In the document provided, the appeal is granted. Only answer with True or False.\"\"\"\n",
    "\n",
    "# apply to df to get openai gpt-4o-mini response:\n",
    "df_sample['openai_outcome'] = df_sample.progress_apply(\n",
    "    lambda x: get_openai_completion(get_prompt(x['unofficial_text'], outcome_question), model = 'gpt-4o-mini'), \n",
    "    axis=1)\n",
    "\n",
    "# len where ollama_outcome != openai_outcome\n",
    "print(\"Differences etween openai and ollama\", len(df_sample[df_sample['ollama_outcome'] != df_sample['openai_outcome']]))\n",
    "\n",
    "# export small sample to json\n",
    "df_sample.to_json(\"RAD_small_sample_outcome_comparison.json\", orient=\"records\", indent = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68784fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "type_question = \"\"\"Multiple choice (return only the number): You are assisting a law professor in categorizing\n",
    "refugee law cases involving persecution based on sexual orientation or gender identity/expression. The professor\n",
    "is interested in the principal claimant's *allegations*, not whether these allegations are true or credible.\n",
    "\n",
    "Your task is to identify the category that best describes the particular social group the principal \n",
    "claimant *alleges* to belong to, based on the information in the document.\n",
    "\n",
    "Select the most appropriate category (if applicable):\n",
    "\n",
    "1. Gay man / homosexual (excluding men who also identify as bisexual or pansexual)\n",
    "2. Lesbian woman (excluding women who also identify as bisexual or pansexual)\n",
    "3. Bisexual / pansexual man (including men who identiy as *both* gay and bisexual)\n",
    "4. Bisexual / pansexual woman  (including women who identify as *both* lesbian and bisexual)\n",
    "5. Transgender / non-binary person  \n",
    "6. Other: A principal claim involving sexual orientation or gender identity/expression that does not fit categories 1-5 (including, e.g. family members or friends of LGBTQ+ individuals)\n",
    "7. Other: A principal claim that does *not* involve sexual orientation or gender identity/expression\n",
    "\n",
    "Be sure to return the number corresponding to your choice in your final answer. Do not return any other text or explanation.\"\"\"\n",
    "\n",
    "# apply to df to get ollama response (use bigger model for accuracy)\n",
    "df['ollama_type'] = df.progress_apply(\n",
    "    lambda x: get_ollama_completion(get_prompt(x['unofficial_text'], type_question), model = 'qwen2.5:72b'), \n",
    "    axis=1)\n",
    "\n",
    "# apply to df to get openai gpt-4o-mini response:\n",
    "df['openai_type'] = df.progress_apply(\n",
    "    lambda x: get_openai_completion(get_prompt(x['unofficial_text'], type_question), model = 'gpt-4o-mini'), \n",
    "    axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f02c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "outcome_question = \"\"\"True or false: In the document provided, the appeal is granted. Only answer with True or False.\"\"\"\n",
    "\n",
    "# apply to df to get ollama response (use smaller model for speed):\n",
    "df['ollama_outcome'] = df.progress_apply(\n",
    "    lambda x: get_ollama_completion(get_prompt(x['unofficial_text'], outcome_question), model = 'qwen2.5:32b'), \n",
    "    axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce463ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export to json\n",
    "df.to_json(\"RAD_SOGIE_type_outcome.json\", orient=\"records\", indent = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d87f303",
   "metadata": {},
   "source": [
    "### Analyze data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5a51f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df_small_sample = pd.read_json(\"RAD_small_sample_outcome_comparison.json\")\n",
    "df_sample = pd.read_json(\"RAD_sample_outcome.json\")\n",
    "df = pd.read_json(\"RAD_SOGIE_type_outcome.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11983322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Differences between openai and ollama: 2 out of 100\n"
     ]
    }
   ],
   "source": [
    "# compare ollama and openai outcomes in df_sample\n",
    "differences = df_small_sample[df_small_sample['ollama_outcome'] != df_small_sample['openai_outcome']]\n",
    "print(\"Differences between openai and ollama:\", len(differences), \"out of\", len(df_small_sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54f3f43",
   "metadata": {},
   "source": [
    "So, we see that the small qwen model agrees with the larger openai model about the\n",
    "outcome of RAD cases 98% of the time, so we'll rely on the small qwen model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dd44cfe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out of 5000 RAD cases in radom sample, 1369 or 27.4 % of cases, are granted\n"
     ]
    }
   ],
   "source": [
    "# Stats on ollama outcomes in df_sample\n",
    "print(\"Out of 5000 RAD cases in radom sample,\", df_sample['ollama_outcome'].value_counts()[\"True\"], \"or\", round(df_sample['ollama_outcome'].value_counts()[\"True\"]/len(df_sample)*100, 1), \"% of cases, are granted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571a01b7",
   "metadata": {},
   "source": [
    "So, the sucess rate on appeal in the random sample of 5,000 RAD cases (across all claim types) is: 27.4%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de7d21dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Types differnces qwen v openai: 309 out of 1717 or 18.0 % of cases\n"
     ]
    }
   ],
   "source": [
    "# number of rows were df.ollama == df.openai\n",
    "print(\"Types differnces qwen v openai:\", len(df[df['ollama_type'] != df['openai_type']]), \"out of\", len(df), \"or\", round(len(df[df['ollama_type'] != df['openai_type']])/len(df)*100, 1), \"% of cases\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1af9d5b",
   "metadata": {},
   "source": [
    "In contrast to outcomes, qwen and openai regularly disagree about what claim type classification best applies to a case (18% of the time). We could try several differnt approaches, including using larger models, using multiple models with voting, using reasoning models, etc. For convenience, we'll just remove the cases where openai and qwen disagree for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9e0ab460",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in df after removing differences: 1408\n"
     ]
    }
   ],
   "source": [
    "# remove rows where there are differences between ollama and openai\n",
    "df = df[df['ollama_type'] == df['openai_type']]\n",
    "df = df.reset_index(drop=True)\n",
    "print(\"Number of rows in df after removing differences:\", len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b749795",
   "metadata": {},
   "source": [
    "Beause we're only interested in SOGIE cases, we will also remove cases where the classification is other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fb21fa03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in df after removing type 6 and 7: 1188\n"
     ]
    }
   ],
   "source": [
    "# remove rows where type is 6 or 7 (i.e. not claim where claimant is LGBTQ+)\n",
    "df = df[(df['ollama_type'] != 6) & (df['ollama_type'] != 7)]\n",
    "df = df.reset_index(drop=True)\n",
    "print(\"Number of rows in df after removing type 6 and 7:\", len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a121bf1",
   "metadata": {},
   "source": [
    "So, that leaves us with 1,188 SOGIE cases. Let's look at outcomes in those cases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5d1d2f45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gay man claims: 521 (43.9%), with success rate of 35.9%\n",
      "Lesbian claims: 192 (16.2%), with success rate of 34.9%\n",
      "Bisexual man claims: 293 (24.7%), with success rate of 37.5%\n",
      "Bisexual woman claims: 177 (14.9%), with success rate of 42.9%\n",
      "Transgeder claims: 5 (0.4%), with success rate of 60.0%\n",
      "\n",
      "All SOGIE claims: 1188, with success rate of 37.3%\n",
      "\n",
      "All claims in random sample for all types: 5,000 with success rate of 27.4 %\n"
     ]
    }
   ],
   "source": [
    "ollama_types = {1:\"Gay man\", 2: \"Lesbian\", 3: \"Bisexual man\", 4: \"Bisexual woman\", 5: \"Transgeder\"}\n",
    "\n",
    "# iterate through dict\n",
    "for ollama_type in ollama_types:\n",
    "    num_claims = len(df[df['ollama_type'] == ollama_type])\n",
    "    percent_claims = len(df[df['ollama_type'] == ollama_type])/len(df)*100\n",
    "    success_rate = len(df[(df['ollama_type'] == ollama_type) & (df['ollama_outcome'] == \"True\")])/num_claims*100\n",
    "    print(f\"{ollama_types[ollama_type]} claims: {num_claims} ({percent_claims:.1f}%), with success rate of {success_rate:.1f}%\")\n",
    "\n",
    "# same but for all types\n",
    "num_claims = len(df)\n",
    "percent_claims = len(df)/len(df)*100\n",
    "success_rate = len(df[df['ollama_outcome'] == \"True\"])/num_claims*100\n",
    "print()\n",
    "print(f\"All SOGIE claims: {num_claims}, with success rate of {success_rate:.1f}%\")\n",
    "print()\n",
    "print(\"All claims in random sample for all types: 5,000 with success rate of\", round(df_sample['ollama_outcome'].value_counts()[\"True\"]/len(df_sample)*100, 1), \"%\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25cf5baf",
   "metadata": {},
   "source": [
    "In the end, we see that SOGIE claims are much more likely to result in successful appeals than other types of claims, suggesting that the refugee determination systems struggles to get these claims right at first instance -- and that different subtypes of SOGIE claims also have varying success rates on appeal."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
